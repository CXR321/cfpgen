{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T11:03:06.481325Z",
     "start_time": "2025-04-24T11:03:06.476480Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "source": [
    "%cd /home/yinj0b/repository/cfp-gen/"
   ],
   "id": "9a8f82d58ec38591",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yinj0b/repository/cfp-gen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yinj0b/anaconda3/envs/cfpgen/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create Functional Protein Dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from subprocess import run\n",
    "import shutil\n",
    "import json\n",
    "import gzip\n",
    "import random\n",
    "from Bio.PDB import PDBParser\n",
    "from Bio.SeqUtils import seq1\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from src.byprot.utils.ontology import Ontology"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def load_pkl_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def save_pkl_file(data, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Updated data saved to {file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def count_go_ipr(new_dataset):\n",
    "    final_ipr_counter = Counter()\n",
    "    final_go_f_counter = Counter()\n",
    "\n",
    "    for entry in new_dataset:\n",
    "        ipr_numbers = entry.get('ipr_numbers', [])\n",
    "        go_numbers_f = entry.get('go_numbers', {}).get('F', [])\n",
    "\n",
    "        final_ipr_counter.update(ipr_numbers)\n",
    "        final_go_f_counter.update(go_numbers_f)\n",
    "\n",
    "    return final_ipr_counter, final_go_f_counter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def filter_by_min_max_count(counter, min_count, max_count):\n",
    "    return set([item for item, count in counter.items() if min_count <= count <= max_count])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def create_filtered_datasets(data, selected_ipr, selected_go_f):\n",
    "\n",
    "    new_dataset = []\n",
    "\n",
    "    for entry in data:\n",
    "        ipr_numbers = set(entry.get('ipr_numbers', []))\n",
    "        go_numbers_f = set(entry.get('go_numbers', {}).get('F', []))\n",
    "\n",
    "        if (not len(ipr_numbers)) or (not len(go_numbers_f)):\n",
    "            continue\n",
    "\n",
    "        if ipr_numbers.issubset(selected_ipr) and go_numbers_f.issubset(selected_go_f):\n",
    "            new_dataset.append(entry)\n",
    "\n",
    "    return new_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def iterative_filtering(data, min_count, max_count, max_iterations=10):\n",
    "\n",
    "    iteration = 0\n",
    "    while iteration < max_iterations:\n",
    "        print(f\"Iteration {iteration + 1}\")\n",
    "\n",
    "        # Analyze the current dataset: count the frequency of IPR and GO(F) numbers\n",
    "        ipr_counter, go_f_counter = count_go_ipr(data)\n",
    "\n",
    "        # Select IPR and GO(F) numbers that appear at least min_count times but no more than max_count times\n",
    "        selected_ipr = filter_by_min_max_count(ipr_counter, min_count, max_count)\n",
    "        selected_go_f = filter_by_min_max_count(go_f_counter, min_count, max_count)\n",
    "\n",
    "        # Generate a filtered dataset containing only the selected IPR and GO(F) entries\n",
    "        new_dataset = create_filtered_datasets(data, selected_ipr, selected_go_f)\n",
    "\n",
    "        # Count the attributes again in the new dataset\n",
    "        final_ipr_counter, final_go_f_counter = count_go_ipr(new_dataset)\n",
    "\n",
    "        if all(min_count <= count <= max_count for count in final_ipr_counter.values()) and \\\n",
    "                all(min_count <= count <= max_count for count in final_go_f_counter.values()):\n",
    "            print(f\"Converged at iteration {iteration + 1}\")\n",
    "            break\n",
    "        data = new_dataset\n",
    "        iteration += 1\n",
    "\n",
    "    new_dataset = deduplicate_by_uniprot_id(new_dataset)\n",
    "    final_ipr_counter, final_go_f_counter = count_go_ipr(new_dataset)\n",
    "\n",
    "    return new_dataset, final_ipr_counter, final_go_f_counter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def update_motif_info(entries, ontology):\n",
    "    for _, sample in tqdm(enumerate(entries), total=len(entries)):\n",
    "        domain_sites = []\n",
    "        seen = set()\n",
    "        for item in sample['domain_sites']:\n",
    "            # Use ('ipr_number', 'domain_id') as a unique identifier\n",
    "            identifier = (item['ipr_number'], item['domain_id'])\n",
    "            if identifier not in seen:\n",
    "                seen.add(identifier)\n",
    "                domain_sites.append(item)\n",
    "\n",
    "        go_terms = sample['go_numbers']['F']\n",
    "        term_desc = []\n",
    "        for go_term in go_terms:\n",
    "            ontology_terms = ontology.get_term(go_term)\n",
    "            if ontology_terms is not None:\n",
    "                term_desc.append(ontology_terms['name'])\n",
    "\n",
    "        motif_segments = []\n",
    "        for term in term_desc:\n",
    "            # Find the closest matching ipr_description for each term_desc\n",
    "            closest_matches = sorted(domain_sites, key=lambda item: SequenceMatcher(None, term, item['ipr_description']).ratio(), reverse=True)[:2]\n",
    "\n",
    "            if not closest_matches:\n",
    "                continue\n",
    "\n",
    "            # Extract start_position and end_position of the matches\n",
    "            start_positions = [item['start_position'] for item in closest_matches]\n",
    "            end_positions = [item['end_position'] for item in closest_matches]\n",
    "\n",
    "            # Calculate the overlapping region\n",
    "            overlap_start = max(start_positions)\n",
    "            overlap_end = min(end_positions)\n",
    "\n",
    "            # If there is an overlap, retrieve the corresponding sequence segment\n",
    "            if overlap_start < overlap_end:\n",
    "                motif_segment = sample['sequence'][overlap_start:overlap_end]\n",
    "                motif_segments.append({\n",
    "                    'go_term': term,\n",
    "                    'motif_segment': motif_segment,\n",
    "                    'start': int(overlap_start),\n",
    "                    'end': int(overlap_end)\n",
    "                })\n",
    "\n",
    "        # Add motif_segments into the sample as a new key-value pair\n",
    "        sample['motif'] = motif_segments\n",
    "\n",
    "    return entries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "def update_domain_info(entries, ipr_map):\n",
    "    for entry in entries:\n",
    "        uniprot_id = entry.get('uniprot_id')\n",
    "        ipr_numbers = entry.get('ipr_numbers', [])\n",
    "\n",
    "        if uniprot_id in ipr_map:\n",
    "            if 'domain_sites' not in entry:\n",
    "                entry['domain_sites'] = []\n",
    "\n",
    "            for ipr_number in ipr_numbers:\n",
    "                domains = [domain for domain in ipr_map[uniprot_id] if domain['ipr_number'] == ipr_number]\n",
    "                if domains:\n",
    "                    entry['domain_sites'].extend(domains)\n",
    "        else:\n",
    "            if 'domain_sites' not in entry:\n",
    "                entry['domain_sites'] = []\n",
    "    return entries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def select_sequences_with_go_count(train_data, final_go_f_counter, min_count=20, target_per_label=20):\n",
    "    \"\"\"\n",
    "    Select sequences from train_data such that each GO category appears at least target_per_label times.\n",
    "    \"\"\"\n",
    "    # Only consider GO labels with counts greater than min_count\n",
    "    valid_go_labels = {label for label, count in final_go_f_counter.items() if count > min_count}\n",
    "\n",
    "    # Track the number of selected sequences for each GO label\n",
    "    go_label_count = defaultdict(int)\n",
    "\n",
    "    # List to store the final selected sequences\n",
    "    selected_sequences = []\n",
    "\n",
    "    # Traverse the training dataset\n",
    "    for entry in tqdm(train_data):\n",
    "        go_labels = entry['go_numbers']['F']  # GO annotations of the current sequence\n",
    "\n",
    "        # Select valid labels (appear more than min_count and have not yet reached target_per_label)\n",
    "        valid_labels_in_entry = [label for label in go_labels if label in valid_go_labels and go_label_count[label] < target_per_label]\n",
    "\n",
    "        # If the current sequence contains any valid labels\n",
    "        if valid_labels_in_entry:\n",
    "            selected_sequences.append(entry)\n",
    "\n",
    "            # Update the count for each GO label\n",
    "            for label in valid_labels_in_entry:\n",
    "                go_label_count[label] += 1\n",
    "\n",
    "            # Remove labels that have already reached the target count\n",
    "            valid_go_labels = {label for label in valid_go_labels if go_label_count[label] < target_per_label}\n",
    "\n",
    "        # Early exit if all labels have reached the target\n",
    "        if not valid_go_labels:\n",
    "            break\n",
    "\n",
    "    # Double check: remove sequences associated with labels that still do not meet the target count\n",
    "    _, fina_go_count = count_go_ipr(selected_sequences)\n",
    "    key_to_rm = [k for k, v in fina_go_count.items() if v < target_per_label]\n",
    "    filtered_sequences = [\n",
    "        entry for entry in selected_sequences\n",
    "        if not any(go_label in key_to_rm for go_label in entry['go_numbers']['F'])\n",
    "    ]\n",
    "\n",
    "    return filtered_sequences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def count_ec(new_dataset):\n",
    "    final_ec_counter = Counter()\n",
    "\n",
    "    for entry in new_dataset:\n",
    "        ec_numbers = entry.get('EC_number', [])\n",
    "\n",
    "        final_ec_counter.update(ec_numbers)\n",
    "\n",
    "    return final_ec_counter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def deduplicate_by_uniprot_id(data):\n",
    "    seen = set()\n",
    "    deduplicated_list = []\n",
    "\n",
    "    for ele in data:\n",
    "        uid = ele['uniprot_id']\n",
    "        if uid not in seen:\n",
    "            deduplicated_list.append(ele)\n",
    "            seen.add(uid)\n",
    "\n",
    "    return deduplicated_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create `cfpgen_general_dataset` based on `uniprot_swissprot_raw.pkl`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Converged at iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing entries: 100%|██████████| 103936/103936 [00:00<00:00, 694173.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique IPRs: 1154\n",
      "Number of unique GO(F): 375\n",
      "Length of filtered Dataset: 103936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to /home/yinj0b/repository/cfp-gen/data-bin/uniprotKB/cfpgen_general_dataset/uniprot_swissprot_go-375_ipr-1154.pkl\n"
     ]
    }
   ],
   "source": [
    "# Get the absolute path of the current script\n",
    "base_dir = os.getcwd()\n",
    "assert base_dir.endswith(\"cfp-gen\"), \"Need to run in cfp-gen root directory\"\n",
    "\n",
    "# Construct the work directory path\n",
    "work_dir = os.path.join(base_dir, \"data-bin/uniprotKB\")\n",
    "\n",
    "# Create the work directory if it does not exist\n",
    "general_dataset_path = os.path.join(work_dir, 'cfpgen_general_dataset')\n",
    "os.makedirs(general_dataset_path, exist_ok=True)\n",
    "\n",
    "raw_pkl_path = os.path.join(work_dir, 'uniprot_swissprot_raw.pkl')\n",
    "raw_data = load_pkl_file(raw_pkl_path)\n",
    "\n",
    "# set minimum number for each class\n",
    "min_count = 100\n",
    "max_count = 20000\n",
    "max_iterations = 10\n",
    "\n",
    "final_dataset, final_ipr_counter, final_go_f_counter = iterative_filtering(raw_data, min_count, max_count, max_iterations)\n",
    "\n",
    "print(f\"Number of unique IPRs: {len(final_ipr_counter)}\")\n",
    "print(f\"Number of unique GO(F): {len(final_go_f_counter)}\")\n",
    "print(f\"Length of filtered Dataset: {len(final_dataset)}\")\n",
    "\n",
    "save_name = os.path.join(general_dataset_path, f'uniprot_swissprot_go-{len(final_go_f_counter)}_ipr-{len(final_ipr_counter)}.pkl')\n",
    "save_pkl_file(final_dataset, save_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add functional domain info add motif info, based on `protein2ipr`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 342/342 [25:38<00:00,  4.50s/it]\n",
      "100%|██████████| 103936/103936 [01:28<00:00, 1170.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to /home/yinj0b/repository/cfp-gen/data-bin/uniprotKB/cfpgen_general_dataset/uniprot_swissprot_go-375_ipr-1154.pkl\n"
     ]
    }
   ],
   "source": [
    "data = load_pkl_file(save_name)\n",
    "\n",
    "protein2ipr_path = os.path.join(work_dir, 'protein2ipr_pkls')\n",
    "\n",
    "# iterate pkls in protein2ipr\n",
    "for pkl_file in tqdm(os.listdir(protein2ipr_path)):\n",
    "    pkl_file_path = os.path.join(protein2ipr_path, pkl_file)\n",
    "\n",
    "    with open(pkl_file_path, 'rb') as f:\n",
    "        ipr_map = pickle.load(f)\n",
    "\n",
    "        data = update_domain_info(data, ipr_map)\n",
    "\n",
    "go_path = os.path.join(work_dir, 'go.obo')\n",
    "ontology = Ontology(go_path, with_rels=True)\n",
    "\n",
    "# make motif based on go main annotations\n",
    "updated_uniprot_entries = update_motif_info(data, ontology)\n",
    "\n",
    "# save updated data\n",
    "save_pkl_file(updated_uniprot_entries, save_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split `train/valid/test` pkls."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 103243/103936 [00:00<00:00, 482393.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data saved to /home/yinj0b/repository/cfp-gen/data-bin/uniprotKB/cfpgen_general_dataset/train.pkl\n",
      "Updated data saved to /home/yinj0b/repository/cfp-gen/data-bin/uniprotKB/cfpgen_general_dataset/test.pkl\n",
      "Updated data saved to /home/yinj0b/repository/cfp-gen/data-bin/uniprotKB/cfpgen_general_dataset/valid.pkl\n",
      "Final dataset sizes:\n",
      "  Train set: 95627 sequences\n",
      "  Validation set: 831 sequences\n",
      "  Test set: 8309 sequences\n"
     ]
    }
   ],
   "source": [
    "train_save_path = os.path.join(general_dataset_path, 'train.pkl')\n",
    "val_save_path = os.path.join(general_dataset_path, 'valid.pkl')\n",
    "test_save_path = os.path.join(general_dataset_path, 'test.pkl')\n",
    "\n",
    "# Load the full dataset\n",
    "data = load_pkl_file(save_name)\n",
    "\n",
    "# Count IPR and GO(F) numbers\n",
    "final_ipr_counter, final_go_f_counter = count_go_ipr(data)\n",
    "\n",
    "# Select sequences for the test set\n",
    "selected_go_test = select_sequences_with_go_count(data, final_go_f_counter, min_count=50, target_per_label=30)\n",
    "test_ids = set([ele['uniprot_id'] for ele in selected_go_test])\n",
    "\n",
    "# Remaining sequences are for training\n",
    "selected_go_train = [ele for ele in data if ele['uniprot_id'] not in test_ids]\n",
    "selected_go_train = deduplicate_by_uniprot_id(selected_go_train)\n",
    "\n",
    "# Save the datasets\n",
    "save_pkl_file(selected_go_train, train_save_path)\n",
    "save_pkl_file(selected_go_test, test_save_path)\n",
    "save_pkl_file(selected_go_test[::10], val_save_path)  # only used for checking loss\n",
    "\n",
    "# Print final dataset sizes\n",
    "print(f\"Final dataset sizes:\")\n",
    "print(f\"  Train set: {len(selected_go_train)} sequences\")\n",
    "print(f\"  Validation set: {len(selected_go_test[::10])} sequences\")\n",
    "print(f\"  Test set: {len(selected_go_test)} sequences\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add backbone coordinate, seq, name info, based on `pdb` and `afdb` database.\n",
    "We need `seq` since it's a little bit different from `sequence` for a given uniprotID, due to the structural constraint.\n",
    "`name` is given based on pdb name."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def extract_seqres(pdb_file):\n",
    "    chain_sequences = {}\n",
    "    with gzip.open(pdb_file, \"rt\") if pdb_file.endswith(\".gz\") else open(pdb_file, \"r\") as f:\n",
    "        for record in SeqIO.parse(f, \"pdb-seqres\"):\n",
    "            chain_sequences[record.id.split(\":\")[1]] = str(record.seq)\n",
    "    return chain_sequences\n",
    "\n",
    "def calculate_similarity(seq1, seq2):\n",
    "    matcher = SequenceMatcher(None, seq1, seq2)\n",
    "    return matcher.ratio()\n",
    "\n",
    "def process_pdb_with_sequence(entry, pdb_name, pdb_file):\n",
    "    sequence = entry.get('sequence')\n",
    "\n",
    "    # Step 1: Extract SEQRES sequences\n",
    "    seqres_sequences = extract_seqres(pdb_file)\n",
    "    num_chains = len(seqres_sequences)\n",
    "    # print('debug', seqres_sequences)\n",
    "\n",
    "    # Step 2: Find the best-matching chain\n",
    "    best_chain = None\n",
    "    best_similarity = 0\n",
    "    for chain_id, seqres_seq in seqres_sequences.items():\n",
    "        similarity = calculate_similarity(sequence, seqres_seq)\n",
    "        if not best_chain or similarity > best_similarity:\n",
    "            best_chain = chain_id\n",
    "            best_similarity = similarity\n",
    "    # print('debug', best_chain)\n",
    "\n",
    "    if best_chain is None:\n",
    "        raise ValueError(\"No matching chain found in PDB file.\")\n",
    "\n",
    "    # print(f\"Best-matching chain: {best_chain} with similarity: {best_similarity:.2f}\")\n",
    "\n",
    "    # Step 3: Extract coordinates\n",
    "    seqres_seq = seqres_sequences[best_chain]\n",
    "    # backbone_coords = extract_backbone_coords(pdb_file, best_chain, seqres_seq)\n",
    "\n",
    "    # Step 4: Add Entry\n",
    "    entry['name'] = f'{pdb_name}.{best_chain}'\n",
    "    entry['num_chains'] = num_chains\n",
    "    entry['seq'] = seqres_seq\n",
    "    # entry['coords'] = backbone_coords\n",
    "    return entry\n",
    "\n",
    "def update_pdb_info(data, pdb2file, afdb2file, pdb_path, afdb_path):\n",
    "    not_found = 0\n",
    "    new_data = []\n",
    "    for entry in tqdm(data):\n",
    "        # fetch pdb_file, pdb_name\n",
    "        pdb_file, pdb_name = '', ''\n",
    "\n",
    "        pdb_ids = entry.get('pdb_ids')\n",
    "        afdb_id = entry.get('afdb')\n",
    "        if pdb_ids:\n",
    "            for pdb_id in pdb_ids:\n",
    "                pdb_id = pdb_id.lower()\n",
    "                if pdb_id in pdb2file:\n",
    "                    pdb_file = pdb2file[pdb_id]\n",
    "                    pdb_file = os.path.join(os.path.dirname(pdb_path), os.path.join(*pdb_file.strip(os.sep).split(os.sep)[-3:]))\n",
    "                    pdb_name = pdb_id\n",
    "                    break\n",
    "\n",
    "        if not pdb_name: # afdb\n",
    "            if afdb_id in afdb2file:\n",
    "                pdb_file = afdb2file[afdb_id]\n",
    "                pdb_file = os.path.join(os.path.dirname(afdb_path), os.path.join(*pdb_file.strip(os.sep).split(os.sep)[-2:]))\n",
    "                pdb_name = afdb_id\n",
    "            else:\n",
    "                print('Warning: PDB/AFDB file not found! Skip: ', entry.get('uniprot_id'))\n",
    "                not_found += 1\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            entry = process_pdb_with_sequence(entry, pdb_name, pdb_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing entry ({entry.get('uniprot_id')}): {e}\")\n",
    "            not_found += 1\n",
    "            continue\n",
    "\n",
    "        new_data.append(entry)\n",
    "\n",
    "    return new_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define filenames and paths\n",
    "splits = ['train', 'valid', 'test']\n",
    "original_paths = [os.path.join(general_dataset_path, f\"{s}.pkl\") for s in splits]\n",
    "new_paths = [os.path.join(general_dataset_path, f\"{s}_bb.pkl\") for s in splits]\n",
    "data_list = [load_pkl_file(p) for p in original_paths]\n",
    "\n",
    "# Load pdb/afdb mappings\n",
    "pdb_path = '/data/junbo/datasets/pdb/pdb_table.csv'\n",
    "afdb_path = '/data/junbo/datasets/afdb_swissprot/af_swissprot_v4_table.csv'\n",
    "pdb2file = pd.read_csv(pdb_path).set_index('PDB_id')['Path'].to_dict()\n",
    "afdb2file = pd.read_csv(afdb_path).set_index('PDB_id')['Path'].to_dict()\n",
    "\n",
    "# Process and save\n",
    "for data, save_path in zip(data_list, new_paths):\n",
    "    print(f\"Processing {save_path} - original size: {len(data)}\")\n",
    "    updated = update_pdb_info(data, pdb2file, afdb2file, pdb_path, afdb_path)\n",
    "    print(f\"Updated size: {len(updated)}\")\n",
    "    save_pkl_file(updated, save_path)\n",
    "    print(f\"Saved to: {save_path}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-cfpgen-py",
   "language": "python",
   "display_name": "Python [conda env:cfpgen]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}